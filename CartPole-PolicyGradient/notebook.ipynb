{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de90966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922328c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: (4,) \n",
      "actions shape:2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "states = env.observation_space.shape\n",
    "actions= env.action_space.n\n",
    "print(f\"state shape: {states} \\nactions shape:{actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf9974",
   "metadata": {},
   "source": [
    "# Making a simple policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53f09b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 , Score: 44.0\n",
      "Episode: 2 , Score: 37.0\n",
      "Episode: 3 , Score: 39.0\n",
      "Episode: 4 , Score: 46.0\n",
      "Episode: 5 , Score: 55.0\n",
      "Episode: 6 , Score: 45.0\n",
      "Episode: 7 , Score: 38.0\n",
      "Episode: 8 , Score: 50.0\n",
      "Episode: 9 , Score: 50.0\n",
      "Episode: 10 , Score: 35.0\n"
     ]
    }
   ],
   "source": [
    "def policy(state):\n",
    "    angle = state[2]\n",
    "    return 0 if angle < 0 else 1 \n",
    "\n",
    "episodes = 10\n",
    "for i in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    score = 0 \n",
    "    done = False\n",
    "    while not done :\n",
    "        action = policy(state)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward \n",
    "        \n",
    "        if done or truncated :\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode: {i+1} , Score: {score}\")\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee84cf2a-c31a-4a4c-8879-be9c9a626b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    Input(states),\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(units=25, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8bb471-12ce-48ca-911e-20e8efe96007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_prob = model(state[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_prob)\n",
    "        y_target = tf.constant([[1.0]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_prob))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    state, reward, done, trauncated, info = env.step(int(action[0,0].numpy()))\n",
    "    return state, reward, done, trauncated, info, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe69e508-793e-4aef-b40b-6790d5bfb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = [] \n",
    "    all_grads = [] \n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = [] \n",
    "        current_grads = [] \n",
    "        state, _ = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            state, reward, done, truncated, info, grads = play_one_step(env, state, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "        return all_rewards, all_grads\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0105f4-4b1d-4f79-9253-999d7e1ae3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2 , -1 , -1):\n",
    "        discounted_rewards[step] += discounted_rewards[step + 1] * discount_factor\n",
    "\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c80f23-3224-476d-b1c3-11fa5f15f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                                 for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429e8d11-6c22-4355-87a4-3f3546ba5588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3531031f-7044-4209-a9ff-51cc792c63ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de87c0bc-c9ea-4f43-ab98-4be8cb757f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150 \n",
    "n_episodes_per_iteration = 10\n",
    "n_max_steps = 200 \n",
    "discount_factor = 0.95 \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "loss_fn = tf.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e66aeba3-3867-416e-b4cd-c7275b688b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Average Reward = 9.00\n",
      "Iteration 2: Average Reward = 31.00\n",
      "Iteration 3: Average Reward = 29.00\n",
      "Iteration 4: Average Reward = 14.00\n",
      "Iteration 5: Average Reward = 22.00\n",
      "Iteration 6: Average Reward = 59.00\n",
      "Iteration 7: Average Reward = 52.00\n",
      "Iteration 8: Average Reward = 21.00\n",
      "Iteration 9: Average Reward = 11.00\n",
      "Iteration 10: Average Reward = 21.00\n",
      "Iteration 11: Average Reward = 47.00\n",
      "Iteration 12: Average Reward = 14.00\n",
      "Iteration 13: Average Reward = 17.00\n",
      "Iteration 14: Average Reward = 20.00\n",
      "Iteration 15: Average Reward = 25.00\n",
      "Iteration 16: Average Reward = 21.00\n",
      "Iteration 17: Average Reward = 22.00\n",
      "Iteration 18: Average Reward = 66.00\n",
      "Iteration 19: Average Reward = 13.00\n",
      "Iteration 20: Average Reward = 22.00\n",
      "Iteration 21: Average Reward = 22.00\n",
      "Iteration 22: Average Reward = 31.00\n",
      "Iteration 23: Average Reward = 23.00\n",
      "Iteration 24: Average Reward = 36.00\n",
      "Iteration 25: Average Reward = 29.00\n",
      "Iteration 26: Average Reward = 27.00\n",
      "Iteration 27: Average Reward = 65.00\n",
      "Iteration 28: Average Reward = 27.00\n",
      "Iteration 29: Average Reward = 18.00\n",
      "Iteration 30: Average Reward = 13.00\n",
      "Iteration 31: Average Reward = 76.00\n",
      "Iteration 32: Average Reward = 37.00\n",
      "Iteration 33: Average Reward = 96.00\n",
      "Iteration 34: Average Reward = 29.00\n",
      "Iteration 35: Average Reward = 69.00\n",
      "Iteration 36: Average Reward = 22.00\n",
      "Iteration 37: Average Reward = 36.00\n",
      "Iteration 38: Average Reward = 59.00\n",
      "Iteration 39: Average Reward = 58.00\n",
      "Iteration 40: Average Reward = 39.00\n",
      "Iteration 41: Average Reward = 121.00\n",
      "Iteration 42: Average Reward = 50.00\n",
      "Iteration 43: Average Reward = 86.00\n",
      "Iteration 44: Average Reward = 51.00\n",
      "Iteration 45: Average Reward = 73.00\n",
      "Iteration 46: Average Reward = 28.00\n",
      "Iteration 47: Average Reward = 60.00\n",
      "Iteration 48: Average Reward = 65.00\n",
      "Iteration 49: Average Reward = 55.00\n",
      "Iteration 50: Average Reward = 59.00\n",
      "Iteration 51: Average Reward = 65.00\n",
      "Iteration 52: Average Reward = 35.00\n",
      "Iteration 53: Average Reward = 137.00\n",
      "Iteration 54: Average Reward = 200.00\n",
      "Iteration 55: Average Reward = 89.00\n",
      "Iteration 56: Average Reward = 157.00\n",
      "Iteration 57: Average Reward = 80.00\n",
      "Iteration 58: Average Reward = 83.00\n",
      "Iteration 59: Average Reward = 59.00\n",
      "Iteration 60: Average Reward = 109.00\n",
      "Iteration 61: Average Reward = 45.00\n",
      "Iteration 62: Average Reward = 57.00\n",
      "Iteration 63: Average Reward = 157.00\n",
      "Iteration 64: Average Reward = 58.00\n",
      "Iteration 65: Average Reward = 64.00\n",
      "Iteration 66: Average Reward = 51.00\n",
      "Iteration 67: Average Reward = 91.00\n",
      "Iteration 68: Average Reward = 59.00\n",
      "Iteration 69: Average Reward = 121.00\n",
      "Iteration 70: Average Reward = 49.00\n",
      "Iteration 71: Average Reward = 53.00\n",
      "Iteration 72: Average Reward = 40.00\n",
      "Iteration 73: Average Reward = 151.00\n",
      "Iteration 74: Average Reward = 52.00\n",
      "Iteration 75: Average Reward = 105.00\n",
      "Iteration 76: Average Reward = 150.00\n",
      "Iteration 77: Average Reward = 156.00\n",
      "Iteration 78: Average Reward = 129.00\n",
      "Iteration 79: Average Reward = 74.00\n",
      "Iteration 80: Average Reward = 88.00\n",
      "Iteration 81: Average Reward = 96.00\n",
      "Iteration 82: Average Reward = 118.00\n",
      "Iteration 83: Average Reward = 95.00\n",
      "Iteration 84: Average Reward = 102.00\n",
      "Iteration 85: Average Reward = 200.00\n",
      "Iteration 86: Average Reward = 158.00\n",
      "Iteration 87: Average Reward = 119.00\n",
      "Iteration 88: Average Reward = 172.00\n",
      "Iteration 89: Average Reward = 180.00\n",
      "Iteration 90: Average Reward = 133.00\n",
      "Iteration 91: Average Reward = 200.00\n",
      "Iteration 92: Average Reward = 200.00\n",
      "Iteration 93: Average Reward = 168.00\n",
      "Iteration 94: Average Reward = 37.00\n",
      "Iteration 95: Average Reward = 200.00\n",
      "Iteration 96: Average Reward = 97.00\n",
      "Iteration 97: Average Reward = 181.00\n",
      "Iteration 98: Average Reward = 200.00\n",
      "Iteration 99: Average Reward = 178.00\n",
      "Iteration 100: Average Reward = 162.00\n",
      "Iteration 101: Average Reward = 191.00\n",
      "Iteration 102: Average Reward = 167.00\n",
      "Iteration 103: Average Reward = 175.00\n",
      "Iteration 104: Average Reward = 125.00\n",
      "Iteration 105: Average Reward = 141.00\n",
      "Iteration 106: Average Reward = 46.00\n",
      "Iteration 107: Average Reward = 27.00\n",
      "Iteration 108: Average Reward = 60.00\n",
      "Iteration 109: Average Reward = 90.00\n",
      "Iteration 110: Average Reward = 133.00\n",
      "Iteration 111: Average Reward = 106.00\n",
      "Iteration 112: Average Reward = 83.00\n",
      "Iteration 113: Average Reward = 89.00\n",
      "Iteration 114: Average Reward = 60.00\n",
      "Iteration 115: Average Reward = 139.00\n",
      "Iteration 116: Average Reward = 108.00\n",
      "Iteration 117: Average Reward = 165.00\n",
      "Iteration 118: Average Reward = 180.00\n",
      "Iteration 119: Average Reward = 189.00\n",
      "Iteration 120: Average Reward = 194.00\n",
      "Iteration 121: Average Reward = 134.00\n",
      "Iteration 122: Average Reward = 101.00\n",
      "Iteration 123: Average Reward = 121.00\n",
      "Iteration 124: Average Reward = 88.00\n",
      "Iteration 125: Average Reward = 105.00\n",
      "Iteration 126: Average Reward = 93.00\n",
      "Iteration 127: Average Reward = 86.00\n",
      "Iteration 128: Average Reward = 90.00\n",
      "Iteration 129: Average Reward = 78.00\n",
      "Iteration 130: Average Reward = 67.00\n",
      "Iteration 131: Average Reward = 70.00\n",
      "Iteration 132: Average Reward = 64.00\n",
      "Iteration 133: Average Reward = 71.00\n",
      "Iteration 134: Average Reward = 84.00\n",
      "Iteration 135: Average Reward = 77.00\n",
      "Iteration 136: Average Reward = 110.00\n",
      "Iteration 137: Average Reward = 134.00\n",
      "Iteration 138: Average Reward = 132.00\n",
      "Iteration 139: Average Reward = 150.00\n",
      "Iteration 140: Average Reward = 163.00\n",
      "Iteration 141: Average Reward = 176.00\n",
      "Iteration 142: Average Reward = 156.00\n",
      "Iteration 143: Average Reward = 200.00\n",
      "Iteration 144: Average Reward = 200.00\n",
      "Iteration 145: Average Reward = 200.00\n",
      "Iteration 146: Average Reward = 200.00\n",
      "Iteration 147: Average Reward = 200.00\n",
      "Iteration 148: Average Reward = 200.00\n",
      "Iteration 149: Average Reward = 200.00\n",
      "Iteration 150: Average Reward = 200.00\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_iteration, n_max_steps, model, loss_fn)\n",
    "    final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    final_grads = [] \n",
    "\n",
    "     # Total reward per episode\n",
    "    episode_returns = [sum(rewards) for rewards in all_rewards]\n",
    "    \n",
    "    # Print the average return across all episodes in this iteration\n",
    "    average_reward = np.mean(episode_returns)\n",
    "    print(f\"Iteration {iteration + 1}: Average Reward = {average_reward:.2f}\")\n",
    "    for variable_idx in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode][step][variable_idx]\n",
    "                                   for episode, rewards in enumerate(final_rewards)\n",
    "                                    for step, final_reward in enumerate(rewards)] , axis=0)\n",
    "        final_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(final_grads , model.trainable_variables))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3edce656-e925-4c74-8003-1e7063adffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1' , render_mode='human')\n",
    "\n",
    "def get_action(model, state):\n",
    "    left_prob = model(state[np.newaxis])\n",
    "    return 0 if left_prob > 0.5 else 1 \n",
    "def gradient_policy(model, env):\n",
    "    state,_ = env.reset()\n",
    "    done = False \n",
    "    score = 0 \n",
    "    while not done:\n",
    "        action = get_action(model, state)\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    print(f\"Final Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb50137f-11e5-4928-ac36-1d1612272038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 500.0\n"
     ]
    }
   ],
   "source": [
    "gradient_policy(model, env)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84207503-3ee4-4a4f-a9e6-17f363027123",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=my_cartpole_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_cartpole_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[0;32m    113\u001b[0m     )\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=my_cartpole_model."
     ]
    }
   ],
   "source": [
    "model.save(\"my_cartpole_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
